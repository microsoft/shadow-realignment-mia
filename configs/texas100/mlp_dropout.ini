dataset=texas100
dataset_size=44000
# Architecture.
architecture=generic-mlp-dropout_6169,1024,512,256,128,100
# Optimization parameters.
optimizer=adam
batch_size=64
learning_rate=0.001
momentum=0.9
weight_decay=0.0
max_num_epochs=100
num_epochs_patience=5
meta_model_learning_rate=0.001
meta_model_num_epochs_patience=1
meta_model_min_learning_rate=1e-5