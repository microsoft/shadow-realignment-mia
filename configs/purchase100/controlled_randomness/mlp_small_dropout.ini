dataset=purchase100
# Architecture.
architecture=generic-mlp-dropout_600,512,256,128,100
# Optimization parameters.
optimizer=adam
batch_size=64
learning_rate=0.001
min_learning_rate=0.0001
momentum=0.9
weight_decay=0.0
max_num_epochs=100
num_epochs_patience=5
dataset_size=20000